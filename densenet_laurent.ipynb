{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# import _pickle as cPickle\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /home/tom/datasets/cifar-10-batches-py/data_batch_1: 10000\n",
      "Loading /home/tom/datasets/cifar-10-batches-py/data_batch_2: 10000\n",
      "Loading /home/tom/datasets/cifar-10-batches-py/data_batch_3: 10000\n",
      "Loading /home/tom/datasets/cifar-10-batches-py/data_batch_4: 10000\n",
      "Loading /home/tom/datasets/cifar-10-batches-py/data_batch_5: 10000\n",
      "Loading /home/tom/datasets/cifar-10-batches-py/test_batch: 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "\n",
    "    u = pickle._Unpickler(fo)\n",
    "    u.encoding = 'latin1'\n",
    "    dict = u.load()\n",
    "    fo.close()\n",
    "    if 'data' in dict:\n",
    "        dict['data'] = dict['data'].reshape((-1, 3, 32, 32)).swapaxes(1, 3).swapaxes(1, 2).reshape(-1, 32*32*3) / 256.\n",
    "\n",
    "    return dict\n",
    "\n",
    "\n",
    "\n",
    "def load_data_one(f):\n",
    "    batch = unpickle(f)\n",
    "    data = batch['data']\n",
    "    labels = batch['labels']\n",
    "    print(\"Loading %s: %d\" % (f, len(data)))\n",
    "    return data, labels\n",
    "\n",
    "def load_data(files, data_dir, label_count):\n",
    "    data, labels = load_data_one(data_dir + '/' + files[0])\n",
    "    for f in files[1:]:\n",
    "        data_n, labels_n = load_data_one(data_dir + '/' + f)\n",
    "        data = np.append(data, data_n, axis=0)\n",
    "        labels = np.append(labels, labels_n, axis=0)\n",
    "    labels = np.array([ [ float(i == label) for i in range(label_count) ] for label in labels ])\n",
    "    return data, labels\n",
    "\n",
    "data_dir = \"/home/tom/datasets/cifar-10-batches-py\"\n",
    "image_size = 32\n",
    "image_dim = image_size * image_size * 3\n",
    "meta = unpickle(data_dir + '/batches.meta')\n",
    "label_names = meta['label_names']\n",
    "label_count = len(label_names)\n",
    "\n",
    "train_files = [ 'data_batch_%d' % d for d in range(1, 6) ]\n",
    "train_data, train_labels = load_data(train_files, data_dir, label_count)\n",
    "pi = np.random.permutation(len(train_data))\n",
    "train_data, train_labels = train_data[pi], train_labels[pi]\n",
    "test_data, test_labels = load_data([ 'test_batch' ], data_dir, label_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = { 'train_data': train_data,\n",
    "        'train_labels': train_labels,\n",
    "        'test_data': test_data,\n",
    "        'test_labels': test_labels }\n",
    "\n",
    "batch_size = 64\n",
    "learning_rate = 0.1\n",
    "train_data, train_labels = data['train_data'], data['train_labels']\n",
    "batch_count = int(len(train_data) / batch_size)\n",
    "batches_data = np.split(train_data[:int(batch_count * batch_size)], batch_count)\n",
    "batches_labels = np.split(train_labels[:int(batch_count * batch_size)], batch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Train: (50000, 3072) (50000, 10)\n",
      "Test: (10000, 3072) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(learning_rate)\n",
    "print(\"Train:\", np.shape(train_data), np.shape(train_labels))\n",
    "print(\"Test:\", np.shape(test_data), np.shape(test_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built graph\n",
      "Batch per epoch:  781\n",
      "1 0 [0.23065871, 0.203125]\n",
      "1 100 [0.1596742, 0.3125]\n",
      "1 200 [0.15700163, 0.46875]\n",
      "1 300 [0.14637868, 0.40625]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def run_in_batch_avg(session, tensors, batch_placeholders, feed_dict={}, batch_size=200):                                                            \n",
    "    res = [ 0 ] * len(tensors)                                                                                                                                                                                     \n",
    "    batch_tensors = [ (placeholder, feed_dict[ placeholder ]) for placeholder in batch_placeholders ]                                        \n",
    "    total_size = len(batch_tensors[0][1])                                                                                                                                                                \n",
    "    batch_count = (total_size + batch_size - 1) / batch_size                                                                                                                         \n",
    "    for batch_idx in range(batch_count):                                                                                                                                                                \n",
    "        current_batch_size = None                                                                                                                                                                                    \n",
    "        for (placeholder, tensor) in batch_tensors:                                                                                                                                                \n",
    "            batch_tensor = tensor[ batch_idx*batch_size : (batch_idx+1)*batch_size ]                                                                                 \n",
    "            current_batch_size = len(batch_tensor)                                                                                                                                                     \n",
    "            feed_dict[placeholder] = tensor[ batch_idx*batch_size : (batch_idx+1)*batch_size ]                                                             \n",
    "        tmp = session.run(tensors, feed_dict=feed_dict)                                                                                                                                        \n",
    "        res = [ r + t * current_batch_size for (r, t) in zip(res, tmp) ]                                                                                                     \n",
    "    return [ r / float(total_size) for r in res ]\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.01, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(input, in_features, out_features, kernel_size, with_bias=False):\n",
    "    W = weight_variable([ kernel_size, kernel_size, in_features, out_features ])\n",
    "    conv = tf.nn.conv2d(input, W, [ 1, 1, 1, 1 ], padding='SAME')\n",
    "    if with_bias:\n",
    "        return conv + bias_variable([ out_features ])\n",
    "    return conv\n",
    "\n",
    "def batch_activ_conv(current, in_features, out_features, kernel_size, is_training, keep_prob):\n",
    "    current = tf.contrib.layers.batch_norm(current, scale=True, is_training=is_training, updates_collections=None)\n",
    "    current = tf.nn.relu(current)\n",
    "    current = conv2d(current, in_features, out_features, kernel_size)\n",
    "    current = tf.nn.dropout(current, keep_prob)\n",
    "    return current\n",
    "\n",
    "def block(input, layers, in_features, growth, is_training, keep_prob):\n",
    "    current = input\n",
    "    features = in_features\n",
    "    for idx in range(layers):\n",
    "        tmp = batch_activ_conv(current, features, growth, 3, is_training, keep_prob)\n",
    "        current = tf.concat((current, tmp), 3)\n",
    "        features += growth\n",
    "    return current, features\n",
    "\n",
    "def avg_pool(input, s):\n",
    "    return tf.nn.avg_pool(input, [ 1, s, s, 1 ], [1, s, s, 1 ], 'VALID')\n",
    "\n",
    "depth = 40\n",
    "weight_decay = 1e-4\n",
    "layers = int((depth - 4) / 3)\n",
    "graph = tf.Graph()\n",
    "\n",
    "\n",
    "with graph.as_default():\n",
    "    xs = tf.placeholder(\"float\", shape=[None, image_dim])\n",
    "    ys = tf.placeholder(\"float\", shape=[None, label_count])\n",
    "    lr = tf.placeholder(\"float\", shape=[])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    is_training = tf.placeholder(\"bool\", shape=[])\n",
    "\n",
    "\n",
    "    current = tf.reshape(xs, [ -1, 32, 32, 3 ])\n",
    "    current = conv2d(current, 3, 16, 3)\n",
    "\n",
    "    current, features = block(current, layers, 16, 12, is_training, keep_prob)\n",
    "    current = batch_activ_conv(current, features, features, 1, is_training, keep_prob)\n",
    "    current = avg_pool(current, 2)\n",
    "    current, features = block(current, layers, features, 12, is_training, keep_prob)\n",
    "    current = batch_activ_conv(current, features, features, 1, is_training, keep_prob)\n",
    "    current = avg_pool(current, 2)\n",
    "    current, features = block(current, layers, features, 12, is_training, keep_prob)\n",
    "\n",
    "    current = tf.contrib.layers.batch_norm(current, scale=True, is_training=is_training, updates_collections=None)\n",
    "    current = tf.nn.relu(current)\n",
    "    current = avg_pool(current, 8)\n",
    "    final_dim = features\n",
    "    current = tf.reshape(current, [ -1, final_dim ])\n",
    "    Wfc = weight_variable([ final_dim, label_count ])\n",
    "    bfc = bias_variable([ label_count ])\n",
    "    ys_ = tf.nn.softmax( tf.matmul(current, Wfc) + bfc )\n",
    "\n",
    "    cross_entropy = -tf.reduce_mean(ys * tf.log(ys_ + 1e-12))\n",
    "    l2 = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\n",
    "    train_step = tf.train.MomentumOptimizer(lr, 0.9, use_nesterov=True).minimize(cross_entropy + l2 * weight_decay)\n",
    "    correct_prediction = tf.equal(tf.argmax(ys_, 1), tf.argmax(ys, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "print(\"Built graph\")\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    print(\"Batch per epoch: \", batch_count)\n",
    "    for epoch in range(1, 1+300):\n",
    "        if epoch == 150: learning_rate = 0.01\n",
    "        if epoch == 225: learning_rate = 0.001\n",
    "        for batch_idx in range(batch_count):\n",
    "            xs_, ys_ = batches_data[batch_idx], batches_labels[batch_idx]\n",
    "            batch_res = session.run([ train_step, cross_entropy, accuracy ],\n",
    "                feed_dict = { xs: xs_,\n",
    "                             ys: ys_, \n",
    "                             lr: learning_rate, \n",
    "                             is_training: True, \n",
    "                             keep_prob: 0.8 })\n",
    "            if batch_idx % 100 == 0: print(epoch, batch_idx, batch_res[1:])\n",
    "\n",
    "        save_path = saver.save(session, 'densenet_%d.ckpt' % epoch)\n",
    "        test_results = run_in_batch_avg(session, [ cross_entropy, accuracy ], [ xs, ys ],\n",
    "                feed_dict = { xs: data['test_data'], ys: data['test_labels'], is_training: False, keep_prob: 1. })\n",
    "        print(epoch, batch_res[1:], test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
