{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# import _pickle as cPickle\n",
    "import pickle\n",
    "import os.path as osp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /home/tom/datasets/cifar-10-batches-py/data_batch_1: 10000\n",
      "Loading /home/tom/datasets/cifar-10-batches-py/data_batch_2: 10000\n",
      "Loading /home/tom/datasets/cifar-10-batches-py/data_batch_3: 10000\n",
      "Loading /home/tom/datasets/cifar-10-batches-py/data_batch_4: 10000\n",
      "Loading /home/tom/datasets/cifar-10-batches-py/data_batch_5: 10000\n",
      "Loading /home/tom/datasets/cifar-10-batches-py/test_batch: 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "\n",
    "    u = pickle._Unpickler(fo)\n",
    "    u.encoding = 'latin1'\n",
    "    dict = u.load()\n",
    "    fo.close()\n",
    "    if 'data' in dict:\n",
    "        dict['data'] = dict['data'].reshape((-1, 3, 32, 32)).swapaxes(1, 3).swapaxes(1, 2).reshape(-1, 32*32*3) / 256.\n",
    "\n",
    "    return dict\n",
    "\n",
    "\n",
    "\n",
    "def load_data_one(f):\n",
    "    batch = unpickle(f)\n",
    "    data = batch['data']\n",
    "    labels = batch['labels']\n",
    "    print(\"Loading %s: %d\" % (f, len(data)))\n",
    "    return data, labels\n",
    "\n",
    "def load_data(files, data_dir, label_count):\n",
    "    data, labels = load_data_one(data_dir + '/' + files[0])\n",
    "    for f in files[1:]:\n",
    "        data_n, labels_n = load_data_one(data_dir + '/' + f)\n",
    "        data = np.append(data, data_n, axis=0)\n",
    "        labels = np.append(labels, labels_n, axis=0)\n",
    "    labels = np.array([ [ float(i == label) for i in range(label_count) ] for label in labels ])\n",
    "    return data, labels\n",
    "\n",
    "data_dir = \"/home/tom/datasets/cifar-10-batches-py\"\n",
    "image_size = 32\n",
    "image_dim = image_size * image_size * 3\n",
    "meta = unpickle(data_dir + '/batches.meta')\n",
    "label_names = meta['label_names']\n",
    "label_count = len(label_names)\n",
    "\n",
    "train_files = [ 'data_batch_%d' % d for d in range(1, 6) ]\n",
    "train_data, train_labels = load_data(train_files, data_dir, label_count)\n",
    "pi = np.random.permutation(len(train_data))\n",
    "train_data, train_labels = train_data[pi], train_labels[pi]\n",
    "test_data, test_labels = load_data([ 'test_batch' ], data_dir, label_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = { 'train_data': train_data,\n",
    "        'train_labels': train_labels,\n",
    "        'test_data': test_data,\n",
    "        'test_labels': test_labels }\n",
    "\n",
    "batch_size = 64\n",
    "learning_rate = 0.1\n",
    "train_data, train_labels = data['train_data'], data['train_labels']\n",
    "batch_count = int(len(train_data) / batch_size)\n",
    "batches_data = np.split(train_data[:int(batch_count * batch_size)], batch_count)\n",
    "batches_labels = np.split(train_labels[:int(batch_count * batch_size)], batch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Train: (50000, 3072) (50000, 10)\n",
      "Test: (10000, 3072) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(learning_rate)\n",
    "print(\"Train:\", np.shape(train_data), np.shape(train_labels))\n",
    "print(\"Test:\", np.shape(test_data), np.shape(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating graph...\n",
      "Built graph\n",
      "Batch per epoch:  781\n",
      "1 0 acc: 0.05\n",
      "1 100 acc: 0.38\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-3fb48ff0bfd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m                              \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                              \u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                              keep_prob: 0.8 })\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tom/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tom/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tom/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/tom/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tom/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run_in_batch_avg(session, tensors, batch_placeholders, feed_dict={}, batch_size=200):\n",
    "    res = [ 0 ] * len(tensors)\n",
    "    batch_tensors = [ (placeholder, feed_dict[ placeholder ]) for placeholder in batch_placeholders ]\n",
    "    total_size = len(batch_tensors[0][1])\n",
    "    batch_count = int((total_size + batch_size - 1) / batch_size)\n",
    "    for batch_idx in range(batch_count):\n",
    "        current_batch_size = None\n",
    "        for (placeholder, tensor) in batch_tensors:\n",
    "            batch_tensor = tensor[ batch_idx*batch_size : (batch_idx+1)*batch_size ]\n",
    "            current_batch_size = len(batch_tensor)\n",
    "            feed_dict[placeholder] = tensor[ batch_idx*batch_size : (batch_idx+1)*batch_size ]\n",
    "        tmp = session.run(tensors, feed_dict=feed_dict)\n",
    "        res = [ r + t * current_batch_size for (r, t) in zip(res, tmp) ]\n",
    "    return [ r / float(total_size) for r in res ]\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.01, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(input, in_features, out_features, kernel_size, with_bias=False):\n",
    "    W = weight_variable([ kernel_size, kernel_size, in_features, out_features ])\n",
    "    conv = tf.nn.conv2d(input, W, [ 1, 1, 1, 1 ], padding='SAME')\n",
    "    if with_bias:\n",
    "        return conv + bias_variable([ out_features ])\n",
    "    return conv\n",
    "\n",
    "def batch_activ_conv(current, in_features, out_features, kernel_size, is_training, keep_prob):\n",
    "    current = tf.contrib.layers.batch_norm(current, scale=True, is_training=is_training, updates_collections=None)\n",
    "    current = tf.nn.relu(current)\n",
    "    current = conv2d(current, in_features, out_features, kernel_size)\n",
    "    current = tf.nn.dropout(current, keep_prob)\n",
    "    return current\n",
    "\n",
    "def block(input, layers, in_features, growth, is_training, keep_prob):\n",
    "    current = input\n",
    "    features = in_features\n",
    "    for idx in range(layers):\n",
    "        tmp = batch_activ_conv(current, features, growth, 3, is_training, keep_prob)\n",
    "        current = tf.concat((current, tmp), 3)\n",
    "        features += growth\n",
    "    return current, features\n",
    "\n",
    "def avg_pool(input, s):\n",
    "    return tf.nn.avg_pool(input, [ 1, s, s, 1 ], [1, s, s, 1 ], 'VALID')\n",
    "\n",
    "depth = 40\n",
    "weight_decay = 1e-4\n",
    "layers = int((depth - 4) / 3)\n",
    "graph = tf.Graph()\n",
    "\n",
    "print(\"Creating graph...\")\n",
    "with graph.as_default():\n",
    "    xs = tf.placeholder(\"float\", shape=[None, image_dim])\n",
    "    ys = tf.placeholder(\"float\", shape=[None, label_count])\n",
    "    lr = tf.placeholder(\"float\", shape=[])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    is_training = tf.placeholder(\"bool\", shape=[])\n",
    "\n",
    "\n",
    "    current = tf.reshape(xs, [ -1, 32, 32, 3 ])\n",
    "    current = conv2d(current, 3, 16, 3)\n",
    "\n",
    "    current, features = block(current, layers, 16, 12, is_training, keep_prob)\n",
    "    current = batch_activ_conv(current, features, features, 1, is_training, keep_prob)\n",
    "    current = avg_pool(current, 2)\n",
    "    current, features = block(current, layers, features, 12, is_training, keep_prob)\n",
    "    current = batch_activ_conv(current, features, features, 1, is_training, keep_prob)\n",
    "    current = avg_pool(current, 2)\n",
    "    current, features = block(current, layers, features, 12, is_training, keep_prob)\n",
    "\n",
    "    current = tf.contrib.layers.batch_norm(current, scale=True, is_training=is_training, updates_collections=None)\n",
    "    current = tf.nn.relu(current)\n",
    "    current = avg_pool(current, 8)\n",
    "    final_dim = features\n",
    "    current = tf.reshape(current, [ -1, final_dim ])\n",
    "    Wfc = weight_variable([ final_dim, label_count ])\n",
    "    bfc = bias_variable([ label_count ])\n",
    "    ys_ = tf.nn.softmax( tf.matmul(current, Wfc) + bfc )\n",
    "\n",
    "    cross_entropy = -tf.reduce_mean(ys * tf.log(ys_ + 1e-12))\n",
    "    l2 = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\n",
    "    loss = cross_entropy + l2 * weight_decay\n",
    "\n",
    "    train_step = tf.train.MomentumOptimizer(lr, 0.9, use_nesterov=True).minimize(loss)\n",
    "    correct_prediction = tf.equal(tf.argmax(ys_, 1), tf.argmax(ys, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    \n",
    "    # Tensorboard\n",
    "    tf.summary.scalar(\"cross_entropy\", cross_entropy)\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    tf.summary.scalar(\"l2\", l2)\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    summary_op =  tf.summary.merge_all()\n",
    "\n",
    "print(\"Built graph\")\n",
    "\n",
    "run_name = \"simple_run\"\n",
    "logs_path = osp.expanduser('~/tb/densenet_laurent/%s' % run_name)\n",
    "summary_writer = tf.summary.FileWriter(logs_path, graph)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    print(\"Batch per epoch: \", batch_count)\n",
    "    for epoch in range(1, 1+300):\n",
    "        if epoch == 150: learning_rate = 0.01\n",
    "        if epoch == 225: learning_rate = 0.001\n",
    "        for batch_idx in range(batch_count):\n",
    "            xs_, ys_ = batches_data[batch_idx], batches_labels[batch_idx]\n",
    "            _, train_acc, summary = session.run([train_step, accuracy, summary_op],\n",
    "                feed_dict = { xs: xs_,\n",
    "                             ys: ys_,\n",
    "                             lr: learning_rate,\n",
    "                             is_training: True,\n",
    "                             keep_prob: 0.8 })\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                summary_writer.add_summary(summary, batch_idx)\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(epoch, batch_idx, \"acc: %.2f\" % (train_acc))\n",
    "                \n",
    "        save_path = saver.save(session, '/tmp/densenet_%d.ckpt' % epoch)\n",
    "        print(\"Saved checkpoint to %s\" % save_path)\n",
    "\n",
    "        test_acc = run_in_batch_avg(session, [accuracy ], [ xs, ys ],\n",
    "                feed_dict = { xs: data['test_data'],\n",
    "                             ys: data['test_labels'],\n",
    "                             is_training: False,\n",
    "                             keep_prob: 1. })\n",
    "\n",
    "        print(\"test_results: %s\" % str(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
