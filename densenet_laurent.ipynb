{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# import _pickle as cPickle\n",
    "import pickle\n",
    "import os.path as osp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /home/tom/datasets/cifar-10-batches-py/data_batch_1: 10000\n",
      "Loading /home/tom/datasets/cifar-10-batches-py/data_batch_2: 10000\n",
      "Loading /home/tom/datasets/cifar-10-batches-py/data_batch_3: 10000\n",
      "Loading /home/tom/datasets/cifar-10-batches-py/data_batch_4: 10000\n",
      "Loading /home/tom/datasets/cifar-10-batches-py/data_batch_5: 10000\n",
      "Loading /home/tom/datasets/cifar-10-batches-py/test_batch: 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "\n",
    "    u = pickle._Unpickler(fo)\n",
    "    u.encoding = 'latin1'\n",
    "    dict = u.load()\n",
    "    fo.close()\n",
    "    if 'data' in dict:\n",
    "        dict['data'] = dict['data'].reshape((-1, 3, 32, 32)).swapaxes(1, 3).swapaxes(1, 2).reshape(-1, 32*32*3) / 256.\n",
    "\n",
    "    return dict\n",
    "\n",
    "\n",
    "\n",
    "def load_data_one(f):\n",
    "    batch = unpickle(f)\n",
    "    data = batch['data']\n",
    "    labels = batch['labels']\n",
    "    print(\"Loading %s: %d\" % (f, len(data)))\n",
    "    return data, labels\n",
    "\n",
    "def load_data(files, data_dir, label_count):\n",
    "    data, labels = load_data_one(data_dir + '/' + files[0])\n",
    "    for f in files[1:]:\n",
    "        data_n, labels_n = load_data_one(data_dir + '/' + f)\n",
    "        data = np.append(data, data_n, axis=0)\n",
    "        labels = np.append(labels, labels_n, axis=0)\n",
    "    labels = np.array([ [ float(i == label) for i in range(label_count) ] for label in labels ])\n",
    "    return data, labels\n",
    "\n",
    "data_dir = \"/home/tom/datasets/cifar-10-batches-py\"\n",
    "image_size = 32\n",
    "image_dim = image_size * image_size * 3\n",
    "meta = unpickle(data_dir + '/batches.meta')\n",
    "label_names = meta['label_names']\n",
    "label_count = len(label_names)\n",
    "\n",
    "train_files = [ 'data_batch_%d' % d for d in range(1, 6) ]\n",
    "train_data, train_labels = load_data(train_files, data_dir, label_count)\n",
    "pi = np.random.permutation(len(train_data))\n",
    "train_data, train_labels = train_data[pi], train_labels[pi]\n",
    "test_data, test_labels = load_data([ 'test_batch' ], data_dir, label_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = { 'train_data': train_data,\n",
    "        'train_labels': train_labels,\n",
    "        'test_data': test_data,\n",
    "        'test_labels': test_labels }\n",
    "\n",
    "batch_size = 64\n",
    "learning_rate = 0.1\n",
    "train_data, train_labels = data['train_data'], data['train_labels']\n",
    "batch_count = int(len(train_data) / batch_size)\n",
    "batches_data = np.split(train_data[:int(batch_count * batch_size)], batch_count)\n",
    "batches_labels = np.split(train_labels[:int(batch_count * batch_size)], batch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Train: (50000, 3072) (50000, 10)\n",
      "Test: (10000, 3072) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(learning_rate)\n",
    "print(\"Train:\", np.shape(train_data), np.shape(train_labels))\n",
    "print(\"Test:\", np.shape(test_data), np.shape(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating graph...\n",
      "Built graph\n",
      "Batch per epoch:  781\n",
      "1 0 acc: 0.11\n"
     ]
    }
   ],
   "source": [
    "def run_in_batch_avg(session, tensors, batch_placeholders, feed_dict={}, batch_size=200):\n",
    "    res = [ 0 ] * len(tensors)\n",
    "    batch_tensors = [ (placeholder, feed_dict[ placeholder ]) for placeholder in batch_placeholders ]\n",
    "    total_size = len(batch_tensors[0][1])\n",
    "    batch_count = int((total_size + batch_size - 1) / batch_size)\n",
    "    for batch_idx in range(batch_count):\n",
    "        current_batch_size = None\n",
    "        for (placeholder, tensor) in batch_tensors:\n",
    "            batch_tensor = tensor[ batch_idx*batch_size : (batch_idx+1)*batch_size ]\n",
    "            current_batch_size = len(batch_tensor)\n",
    "            feed_dict[placeholder] = tensor[ batch_idx*batch_size : (batch_idx+1)*batch_size ]\n",
    "        tmp = session.run(tensors, feed_dict=feed_dict)\n",
    "        res = [ r + t * current_batch_size for (r, t) in zip(res, tmp) ]\n",
    "    return [ r / float(total_size) for r in res ]\n",
    "\n",
    "def variable_summaries(var):\n",
    "  \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "  with tf.name_scope('summaries'):\n",
    "    mean = tf.reduce_mean(var)\n",
    "    tf.summary.scalar('mean', mean)\n",
    "    with tf.name_scope('stddev'):\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "    tf.summary.scalar('stddev', stddev)\n",
    "    tf.summary.scalar('max', tf.reduce_max(var))\n",
    "    tf.summary.scalar('min', tf.reduce_min(var))\n",
    "    tf.summary.histogram('histogram', var)\n",
    "\n",
    "def weight_variable(shape):\n",
    "    with tf.name_scope('weights'):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "        var = tf.Variable(initial)\n",
    "#         variable_summaries(var)\n",
    "    return var\n",
    "\n",
    "def bias_variable(shape):\n",
    "    with tf.name_scope('biases'):\n",
    "        initial = tf.constant(0.01, shape=shape)\n",
    "        var =  tf.Variable(initial)\n",
    "#         variable_summaries(var)\n",
    "    return var\n",
    "    \n",
    "def conv2d(input, in_features, out_features, kernel_size, with_bias=False):\n",
    "    W = weight_variable([ kernel_size, kernel_size, in_features, out_features ])\n",
    "    conv = tf.nn.conv2d(input, W, [ 1, 1, 1, 1 ], padding='SAME')\n",
    "    if with_bias:\n",
    "        return conv + bias_variable([ out_features ])\n",
    "    return conv\n",
    "\n",
    "def batch_activ_conv(current, in_features, out_features, kernel_size, is_training, keep_prob):\n",
    "    current = tf.contrib.layers.batch_norm(current, scale=True, is_training=is_training, updates_collections=None)\n",
    "    current = tf.nn.relu(current)\n",
    "    current = conv2d(current, in_features, out_features, kernel_size)\n",
    "    current = tf.nn.dropout(current, keep_prob)\n",
    "    return current\n",
    "\n",
    "def block(input, layers, in_features, growth, is_training, keep_prob):\n",
    "    current = input\n",
    "    features = in_features\n",
    "    for idx in range(layers):\n",
    "        tmp = batch_activ_conv(current, features, growth, 3, is_training, keep_prob)\n",
    "        current = tf.concat((current, tmp), 3)\n",
    "        features += growth\n",
    "    return current, features\n",
    "\n",
    "def avg_pool(input, s):\n",
    "    return tf.nn.avg_pool(input, [ 1, s, s, 1 ], [1, s, s, 1 ], 'VALID')\n",
    "\n",
    "depth = 40\n",
    "weight_decay = 1e-4\n",
    "layers = int((depth - 4) / 3)\n",
    "graph = tf.Graph()\n",
    "\n",
    "print(\"Creating graph...\")\n",
    "with graph.as_default():\n",
    "    xs = tf.placeholder(\"float\", shape=[None, image_dim])\n",
    "    ys = tf.placeholder(\"float\", shape=[None, label_count])\n",
    "    lr = tf.placeholder(\"float\", shape=[])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    is_training = tf.placeholder(\"bool\", shape=[])\n",
    "\n",
    "\n",
    "    current = tf.reshape(xs, [ -1, 32, 32, 3 ])\n",
    "    current = conv2d(current, 3, 16, 3)\n",
    "\n",
    "    current, features = block(current, layers, 16, 12, is_training, keep_prob)\n",
    "    current = batch_activ_conv(current, features, features, 1, is_training, keep_prob)\n",
    "    current = avg_pool(current, 2)\n",
    "    current, features = block(current, layers, features, 12, is_training, keep_prob)\n",
    "    current = batch_activ_conv(current, features, features, 1, is_training, keep_prob)\n",
    "    current = avg_pool(current, 2)\n",
    "    current, features = block(current, layers, features, 12, is_training, keep_prob)\n",
    "\n",
    "    current = tf.contrib.layers.batch_norm(current, scale=True, is_training=is_training, updates_collections=None)\n",
    "    current = tf.nn.relu(current)\n",
    "    current = avg_pool(current, 8)\n",
    "    final_dim = features\n",
    "    current = tf.reshape(current, [ -1, final_dim ])\n",
    "    Wfc = weight_variable([ final_dim, label_count ])\n",
    "    bfc = bias_variable([ label_count ])\n",
    "    ys_ = tf.nn.softmax( tf.matmul(current, Wfc) + bfc )\n",
    "\n",
    "    cross_entropy = -tf.reduce_mean(ys * tf.log(ys_ + 1e-12))\n",
    "    l2 = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\n",
    "    loss = cross_entropy + l2 * weight_decay\n",
    "\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    train_step = tf.train.MomentumOptimizer(lr, 0.9, use_nesterov=True).minimize(loss, global_step=global_step)\n",
    "    correct_prediction = tf.equal(tf.argmax(ys_, 1), tf.argmax(ys, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    \n",
    "    # Tensorboard\n",
    "    with tf.name_scope('losses'):\n",
    "        tf.summary.scalar(\"cross_entropy\", cross_entropy)\n",
    "        tf.summary.scalar(\"accuracy\", accuracy)\n",
    "        tf.summary.scalar(\"l2\", l2)\n",
    "        tf.summary.scalar(\"loss\", loss)\n",
    "    summary_op =  tf.summary.merge_all()\n",
    "    \n",
    "\n",
    "print(\"Built graph\")\n",
    "\n",
    "run_name = \"vanilla-nosum\"\n",
    "logs_path = osp.expanduser('~/tb/densenet_laurent/%s' % run_name)\n",
    "summary_writer = tf.summary.FileWriter(logs_path, graph)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    print(\"Batch per epoch: \", batch_count)    \n",
    "    for epoch in range(1, 1+300):\n",
    "        if epoch == 150: learning_rate = 0.01\n",
    "        if epoch == 225: learning_rate = 0.001\n",
    "        for batch_idx in range(batch_count):\n",
    "            xs_, ys_ = batches_data[batch_idx], batches_labels[batch_idx]\n",
    "            _, train_acc, summary, step = session.run([train_step, accuracy, summary_op, global_step],\n",
    "                feed_dict = { xs: xs_,\n",
    "                             ys: ys_,\n",
    "                             lr: learning_rate,\n",
    "                             is_training: True,\n",
    "                             keep_prob: 0.8 })\n",
    "            summary_writer.add_summary(summary, step)\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(epoch, batch_idx, \"acc: %.2f\" % (train_acc))\n",
    "                \n",
    "        save_path = saver.save(session, '/tmp/densenet_%d.ckpt' % epoch)\n",
    "        print(\"Saved checkpoint to %s\" % save_path)\n",
    "\n",
    "        test_acc = run_in_batch_avg(session, [accuracy ], [ xs, ys ],\n",
    "                feed_dict = { xs: data['test_data'],\n",
    "                             ys: data['test_labels'],\n",
    "                             is_training: False,\n",
    "                             keep_prob: 1. })\n",
    "\n",
    "        print(\"test_results: %s\" % str(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
