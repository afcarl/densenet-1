{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.core.debugger import Tracer\n",
    "from six.moves import cPickle \n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "\n",
    "sns.set(font='DejaVu Sans')\n",
    "mpl.rcParams['font.family'] = 'DejaVu Sans'\n",
    "\n",
    "mpl.rcParams['font.sans-serif'].insert(0, 'DejaVu Sans')\n",
    "mpl.rcParams['font.family'] = 'sans-serif'\n",
    "mpl.rcParams['figure.figsize'] = 12, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/nbuser/cifar-10-batches-py/data_batch_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a0e252ea8f70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/nbuser/cifar-10-batches-py/data_batch_%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mim_batches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mall_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-a0e252ea8f70>\u001b[0m in \u001b[0;36mload_batch\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# https://stackoverflow.com/questions/35995999/why-cifar-10-images-are-not-displayed-properly-using-matplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mdatadict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/nbuser/cifar-10-batches-py/data_batch_1'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fetch data with the following\n",
    "#\n",
    "#   wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "#   tar xzf cifar-10-python.tar.gz\n",
    "# \n",
    "\n",
    "def load_batch(path):\n",
    "    # https://stackoverflow.com/questions/35995999/why-cifar-10-images-are-not-displayed-properly-using-matplotlib\n",
    "    f = open(path, 'rb')\n",
    "    datadict = cPickle.load(f,encoding='latin1')\n",
    "    f.close()\n",
    "    X = datadict[\"data\"] \n",
    "    Y = datadict['labels']\n",
    "    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float32\")\n",
    "    X = X.reshape(10000, -1)\n",
    "    Y = np.array(Y)\n",
    "    return X, Y\n",
    "\n",
    "im_batches = []\n",
    "all_labels = []\n",
    "for i in [1,2,3,4,5]:\n",
    "    ims, labs = load_batch('/home/nbuser/cifar-10-batches-py/data_batch_%s' % i)\n",
    "    im_batches.append(ims)\n",
    "    all_labels.append(labs)\n",
    "    \n",
    "all_raw_images = np.vstack(im_batches)\n",
    "all_labels = np.hstack(all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_names = [    \n",
    "'airplane',\n",
    "'automobile',\n",
    "'bird',\n",
    "'cat',\n",
    "'deer',\n",
    "'dog',\n",
    "'frog',\n",
    "'horse',\n",
    "'ship',\n",
    "'truck',\n",
    "]\n",
    "#Visualizing CIFAR 10\n",
    "\n",
    "def show_images(images):\n",
    "    # TODO: Add flag to unwhiten?\n",
    "    fig, axes1 = plt.subplots(5,5,figsize=(3,3))\n",
    "    for j in range(5):\n",
    "        for k in range(5):\n",
    "            i = np.random.choice(range(len(images)))\n",
    "            axes1[j][k].set_axis_off()\n",
    "            data = images[i:i+1][0].reshape(32, 32, 3).astype('uint8')\n",
    "            axes1[j][k].imshow(data)\n",
    "                \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_raw_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d21e2c2a86bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_raw_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mTRAINING_MEAN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_raw_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mTRAINING_STD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_raw_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_raw_images' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(all_raw_images))\n",
    "\n",
    "TRAINING_MEAN = np.mean(all_raw_images)\n",
    "TRAINING_STD = np.std(all_raw_images)\n",
    "\n",
    "def whiten(images):\n",
    "    images = images - TRAINING_MEAN\n",
    "    return images / TRAINING_STD\n",
    "    \n",
    "def unwhiten(images):\n",
    "    images = images * TRAINING_STD\n",
    "    return images + TRAINING_MEAN\n",
    "    \n",
    "images = whiten(all_raw_images)[:45000]\n",
    "labels = all_labels[:45000]\n",
    "\n",
    "val_images = whiten(all_raw_images)[45000:]\n",
    "val_labels = all_labels[45000:]\n",
    "\n",
    "all_raw_test_images, test_labels = load_batch('/home/nbuser/cifar-10-batches-py/test_batch')\n",
    "test_images = whiten(all_raw_test_images)\n",
    "test_labels = np.asarray(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b0568e2d5c0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshow_whitened_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munwhiten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"uint8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"
     ]
    }
   ],
   "source": [
    "print(np.mean(images[0]), np.std(images[0]))\n",
    "\n",
    "def show_whitened_image(image, label=None):\n",
    "    image = unwhiten(image)\n",
    "    image = image.reshape(32, 32, 3).astype(\"uint8\")\n",
    "    with sns.axes_style(\"white\"):\n",
    "        plt.imshow(image, interpolation=\"bicubic\")\n",
    "    if label:\n",
    "        plt.title(label_names[label])\n",
    "    plt.show()\n",
    "\n",
    "for i in range(1):\n",
    "    show_whitened_image(images[i], labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'all_raw_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-65ee6582aa71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Raw\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_raw_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_raw_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Whitened\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_raw_images' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Raw\")\n",
    "print(np.mean(all_raw_images))\n",
    "print(np.var(all_raw_images))\n",
    "\n",
    "print(\"Whitened\")\n",
    "print(np.mean(images))\n",
    "print(np.var(images))\n",
    "\n",
    "show_images(unwhiten(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitened Test\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3e79e8eb2e11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Whitened Test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mshow_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munwhiten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_images' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Whitened Test\")\n",
    "print(np.mean(test_images))\n",
    "print(np.var(test_images))\n",
    "\n",
    "show_images(unwhiten(test_images))\n",
    "print(len(test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n",
      "['/gpu:0']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "print(get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_graph_stats(verbose=False):\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        # shape is an array of tf.Dimension\n",
    "        shape = variable.get_shape()\n",
    "        if verbose:\n",
    "            print('')\n",
    "            print(shape)\n",
    "        variable_parametes = 1\n",
    "        for dim in shape:\n",
    "            variable_parametes *= dim.value\n",
    "        if verbose:\n",
    "            print(variable_parametes)\n",
    "        total_parameters += variable_parametes\n",
    "\n",
    "    print(\"Total params = %s\" % total_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params = 9672778\n",
      "Graph ready\n",
      "Starting training...\n",
      "\n",
      "loss: 2.923, acc: 8.6%\n",
      "loss: 2.369, acc: 8.6%\n",
      "loss: 2.351, acc: 10.8%\n",
      "loss: 2.324, acc: 10.2%\n",
      "loss: 2.204, acc: 17.2%\n",
      "loss: 2.153, acc: 17.3%\n",
      "loss: 2.052, acc: 18.5%\n",
      "loss: 2.069, acc: 22.5%\n",
      "\n",
      "epoch: 0   base_lr: 0.0002   loss: 2.01423, elapsed_time: 0:01:09.759790\n",
      "225 correct out of 1000 => accuracy: 0.225\n",
      "\n",
      "loss: 1.965, acc: 24.9%\n",
      "loss: 1.883, acc: 25.9%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b6f07171782f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0my_train\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mannealing_lr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         })\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Get determinism\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "# Build our models\n",
    "input_size = 3*32*32\n",
    "batch_size = 128 # To use GPU correctly\n",
    "\n",
    "\n",
    "# VGG based on http://torch.ch/blog/2015/07/30/cifar.html\n",
    "\n",
    "xavier = tf.contrib.layers.xavier_initializer()\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "lr = tf.placeholder(tf.float32)\n",
    "\n",
    "def conv_bn_relu(x, in_channels, out_channels, scope, dropout=True):\n",
    "    with tf.variable_scope(scope):\n",
    "        W = tf.get_variable(\"W\", shape=[3, 3, in_channels, out_channels], initializer=xavier)\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[out_channels])) \n",
    "\n",
    "    conved = tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
    "    # TODO: add spatial batchnorm before relu. Bias can be omitted when using batchnorm\n",
    "    out = tf.nn.relu(conved + b)\n",
    "    if dropout:\n",
    "        out = tf.nn.dropout(out, keep_prob=keep_prob)\n",
    "    return out\n",
    "\n",
    "def conv_group(x, in_channels, out_channels, scope):\n",
    "    with tf.variable_scope(scope):\n",
    "        conv = conv_bn_relu(x, in_channels, out_channels, \"conv_1\")\n",
    "        # TODO: Add variable layer of convs in a group\n",
    "        conv = conv_bn_relu(conv, out_channels, out_channels, \"conv_2\", dropout=False)\n",
    "        pooled = max_pool_2x2(conv) \n",
    "    return pooled\n",
    "\n",
    "def single_h_mlp(x, in_size, out_size, h_size, scope=\"mlp\"):\n",
    "    with tf.variable_scope(scope):\n",
    "        W1 = tf.get_variable(\"W1\", shape=[in_size, h_size], initializer=xavier)\n",
    "        b1 = tf.Variable(tf.constant(0.1, shape=[h_size]))\n",
    "\n",
    "        W2 = tf.get_variable(\"W2\", shape=[h_size, out_size], initializer=xavier)\n",
    "        b2 = tf.Variable(tf.constant(0.1, shape=[out_size]))\n",
    "\n",
    "    x = tf.nn.dropout(x, keep_prob=keep_prob)\n",
    "\n",
    "    h_layer = tf.matmul(x, W1) + b1\n",
    "    # TODO: Add batchnorm here\n",
    "    h_layer = tf.nn.relu(h_layer)\n",
    "    h_layer = tf.nn.dropout(h_layer, keep_prob=keep_prob)\n",
    "    \n",
    "    out = tf.matmul(h_layer, W2) + b2\n",
    "    return out\n",
    "        \n",
    "        \n",
    "x = tf.placeholder(tf.float32, shape=(None, input_size))\n",
    "y_train = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 32, 32, 3])\n",
    "\n",
    "# Conv layers\n",
    "pool_1 = conv_group(x_image, 3, 64, 'group_1') # (16x16x64)\n",
    "pool_2 = conv_group(pool_1, 64, 128, 'group_2') # (8x8x128)\n",
    "pool_3 = conv_group(pool_2, 128, 256, 'group_3') # (4x4x256)\n",
    "pool_4 = conv_group(pool_3, 256, 512, 'group_4') # (2x2x512)\n",
    "conv_out = conv_group(pool_4, 512, 512, 'group_5') # (1x1x512)\n",
    "\n",
    "conv_out_flat = tf.reshape(conv_out, [-1, 512])\n",
    "\n",
    "logits = single_h_mlp(conv_out_flat, 512, 10, 512)\n",
    "\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_train, logits=logits)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# Optimizers\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    # Ensures that we execute the update_ops before performing the train_step\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "#         optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "#################################\n",
    "### Finished defining graph\n",
    "#################################\n",
    "\n",
    "print_graph_stats(False)\n",
    "print(\"Graph ready\")\n",
    "\n",
    "\n",
    "def check_accuracy(test_ims, test_ys, sess):    \n",
    "    total_correct = 0\n",
    "    for i in range(len(test_ims) // batch_size):\n",
    "        logits_out = sess.run(logits, feed_dict={\n",
    "            x: test_ims[i*batch_size:(i+1)*batch_size],\n",
    "            keep_prob: 1.0,\n",
    "        })\n",
    "        y_pred = np.argmax(logits_out, 1)\n",
    "        correct = (y_pred - test_ys[i*batch_size:(i+1)*batch_size]) == 0\n",
    "        total_correct += correct\n",
    "\n",
    "    acc = np.sum(total_correct) / len(test_ys)\n",
    "\n",
    "    message = \"%s correct out of %s => accuracy: %s\" % ( np.sum(total_correct), len(test_ys), acc)\n",
    "    return acc, message\n",
    "    \n",
    "#################################\n",
    "### Start training\n",
    "#################################\n",
    "\n",
    "report = {\n",
    "    'loss': [],\n",
    "    'val_acc': [],\n",
    "}\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "annealing_lr = 2*10e-5\n",
    "sess = tf.InteractiveSession()\n",
    "def sample(images, labels, n=50):\n",
    "    sample_indices = random.sample(range(len(images)), n)\n",
    "    return images[sample_indices], labels[sample_indices]\n",
    "\n",
    "# Initialize\n",
    "sess.run(tf.global_variables_initializer()) # This always sets the same values\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "start = datetime.now()\n",
    "for epoch in range(n_epochs):\n",
    "#     if epoch % 25 == 0:\n",
    "#         annealing_lr /= 2.\n",
    "#         print(\"Annealed base lr to %s\" % annealing_lr)\n",
    "        \n",
    "    iters_per_epoch = 50000 // batch_size\n",
    "    for i in range(iters_per_epoch):\n",
    "        train_ims, train_labels = sample(images, labels, batch_size)\n",
    "\n",
    "        _, l = sess.run([train_op, loss], feed_dict={\n",
    "            x: train_ims,\n",
    "            y_train: train_labels,\n",
    "            keep_prob: 0.5,\n",
    "            lr: annealing_lr\n",
    "        })\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            report['loss'].append(l)\n",
    "            acc, acc_msg = check_accuracy(*sample(val_images, val_labels, 1000), sess)\n",
    "\n",
    "            report['val_acc'].append(acc)\n",
    "            print(\"loss: %.3f, acc: %.1f%%\" % (l, acc*100))\n",
    "            \n",
    "    training_time = datetime.now() - start\n",
    "    print(\"\\nepoch: %s   base_lr: %s   loss: %s, elapsed_time: %s\" % (epoch, annealing_lr, l, training_time))\n",
    "    print(acc_msg)\n",
    "    print()\n",
    "\n",
    "print(\"Training time: %s\" % training_time)\n",
    "\n",
    "acc, _ = check_accuracy(test_images, test_labels, sess)\n",
    "print(\"Final test accuracy: %.1f%%\" % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def smooth(vals):\n",
    "    window = 1\n",
    "    return np.convolve(vals, np.ones(window)/window, mode='valid')\n",
    "\n",
    "xs = [log / 20 for log in range(len(report['loss']))]\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.plot(xs, smooth(report['loss']))\n",
    "ax1.set_title(\"Loss\")\n",
    "ax2.plot(xs, smooth(np.log(report['loss'])))\n",
    "ax2.set_title(\"Log Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Validation accuracy\")\n",
    "ys = smooth(report['val_acc'][20:])\n",
    "plt.plot(xs, ys)\n",
    "plt.show()\n",
    "\n",
    "print(\"Training time: %s\" % training_time)\n",
    "print(\"Final validation accuracy: %.1f%%\" % (np.mean(report['val_acc'][-10:]) *100))\n",
    "print(\"Final test accuracy: %.1f%%\" % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "n = 30\n",
    "offset = 20\n",
    "fig, axes1 = plt.subplots(n,11, figsize=(16, 3*n/2))\n",
    "for k in range(10):\n",
    "    ax = axes1[0][k+1] \n",
    "    ax.set_title(label_names[k])\n",
    "\n",
    "    \n",
    "render_ims = test_images[int(j) + offset: int(j) + offset + n]\n",
    "\n",
    "saliency_maps, logits_out = sess.run([h_conv3, logits], feed_dict={\n",
    "    x: render_ims,\n",
    "    keep_prob: 1.0,\n",
    "})\n",
    "\n",
    "mu, sigma = np.mean(saliency_maps), np.std(saliency_maps)\n",
    "print(mu, sigma)\n",
    "for j in range(n):\n",
    "    im = unwhiten(render_ims[j])\n",
    "\n",
    "    data = im.reshape(32, 32, 3).astype('uint8')\n",
    "    axes1[j][0].set_axis_off()\n",
    "    axes1[j][0].imshow(data, interpolation=\"bicubic\")    \n",
    "    y_pred = np.argmax(logits_out[j])\n",
    "        \n",
    "    for k in range(10):\n",
    "        ax = axes1[j][k+1] \n",
    "        if y_pred == k:\n",
    "            ax.grid(False)\n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_edgecolor('blue')\n",
    "                spine.set_linewidth(3.0)\n",
    "\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_xticklabels([])                \n",
    "        else:\n",
    "            ax.set_axis_off()                \n",
    "        \n",
    "        # Clip so that it renders nicely\n",
    "        rng_min = mu - (0.2 * sigma)\n",
    "        rng_max = mu + (1.5 * sigma)\n",
    "        sal_map = np.clip(saliency_maps[j, :, :, k], rng_min, rng_max)      \n",
    "        ax.imshow(sal_map, cmap=\"gray\", \n",
    "                  vmin=rng_min, \n",
    "                  vmax=rng_max,\n",
    "                  interpolation=\"bicubic\",\n",
    "                 )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
